{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "rm: `/h1': No such file or directory\n",
      "rm: `/h2': No such file or directory\n"
     ]
    }
   ],
   "source": [
    "!hdfs dfs -rm -R /h1\n",
    "!hdfs dfs -rm -R /h2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Deleted /h1\n",
      "Deleted /h2\n",
      "2020-10-25 04:40:48,035 INFO sasl.SaslDataTransferClient: SASL encryption trust check: localHostTrusted = false, remoteHostTrusted = false\n"
     ]
    }
   ],
   "source": [
    "!hdfs dfs -rm -R /h1\n",
    "!hdfs dfs -rm -R /h2\n",
    "!hadoop fs -mkdir -p /h1\n",
    "!hadoop fs -put data2/testbfs.txt /h1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING: Use of this script to execute dfs is deprecated.\n",
      "WARNING: Attempting to execute replacement \"hdfs dfs\" instead.\n",
      "\n",
      "Found 1 items\n",
      "-rw-r--r--   3 root supergroup        172 2020-10-25 04:40 /h1/testbfs.txt\n"
     ]
    }
   ],
   "source": [
    "!hadoop dfs -ls /h1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Writing mapper.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile mapper.py\n",
    "#!/opt/bitnami/python/bin/python\n",
    "# -*-coding:utf-8 -*\n",
    "import sys\n",
    "import re\n",
    "\n",
    "black_count = 0\n",
    "for line in sys.stdin: # reads from stdin\n",
    "    line = line.strip()\n",
    "    if(\"GRAY\" in line):\n",
    "        black_line = line.replace(\"GRAY\",\"BLACK\")\n",
    "        print(black_line)\n",
    "        print(\"reporter:counter:%s,%s,%d\" % ('example-info','black_count',1), file=sys.stderr)\n",
    "        source, info = line.split()\n",
    "        kids,score,color,parent = info.split(\"|\")\n",
    "        score = int(score)+1\n",
    "        kids = kids.split(\",\")\n",
    "        for kid in kids:\n",
    "            print(kid+\"\\tnull|\"+str(score)+\"|GRAY|\"+source)\n",
    "    else:\n",
    "        print(line)\n",
    "        if(\"BLACK\" in line):\n",
    "            print(\"reporter:counter:%s,%s,%d\" % ('example-info','black_count',1), file=sys.stderr)    \n",
    "        \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting reducer.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile reducer.py\n",
    "#!/opt/bitnami/python/bin/python\n",
    "# -*-coding:utf-8 -*\n",
    "\n",
    "import sys\n",
    "import random\n",
    "\n",
    "node = dict()\n",
    "for line in sys.stdin:\n",
    "    line = line.strip()\n",
    "#     print(line)\n",
    "    source,info=line.split()\n",
    "\n",
    "# #     if(\"5\" in source):\n",
    "    kids,score,color,parent = info.split(\"|\")\n",
    "    if(not node): #fill node if currently empty\n",
    "        node['source'] = source\n",
    "        node['kids'] = kids\n",
    "        node['score'] = score\n",
    "        node['color'] = color\n",
    "        node['parent'] = parent\n",
    "    else:\n",
    "        score_updated = False\n",
    "            #dont need to touch the source field anymore. partitioner already groups by source field\n",
    "            #only replace kids if find node copy with kids\n",
    "        if(kids != \"null\"):\n",
    "            node['kids'] = kids\n",
    "            \n",
    "#         if(\"null\" not in kids):\n",
    "#             node['kids'] = kids\n",
    "            #ignore max value lines\n",
    "\n",
    "        if(score.isnumeric()):\n",
    "            if(node['score'] == \"Integer.MAX_VALUE\"):\n",
    "                node['score'] = score\n",
    "                score_updated = True\n",
    "            elif(int(node['score']) > int(score)):\n",
    "                node['score'] = score\n",
    "                score_updated = True\n",
    "#         if(\"Integer.MAX_VALUE\" not in score):\n",
    "#                 #check if a lower score found. Replace if true\n",
    "#             if(int(node['score']) > int(score)):\n",
    "#                 node['score'] = score\n",
    "            \n",
    "            #take darkest color\n",
    "        if(\"WHITE\" == node['color'] and \"WHITE\" != color):#if color is darker than white, replace\n",
    "            node['color'] = color\n",
    "        elif(\"GRAY\" == node['color'] and \"BLACK\" == color):#if color is darker than gray, replace\n",
    "            node['color'] = color\n",
    "                \n",
    "            #if incoming parent is null or source do nothing, parent needs to be a number to compare\n",
    "            # also only change parent if score was changed.\n",
    "\n",
    "            \n",
    "        if(parent == 'source'):\n",
    "            node['parent'] = 'source'\n",
    "            break\n",
    "        if(score_updated == True):\n",
    "            if(node['parent'] == 'null' and parent.isnumeric()):\n",
    "                node['parent'] = parent\n",
    "            elif(node['parent'].isnumeric() and parent.isnumeric()):\n",
    "                if(int(node['parent']) > int(parent)):\n",
    "                    node['parent'] = parent\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "print(node['source']+\"\\t\"+node['kids']+\"|\"+node['score']+\"|\"+node['color']+\"|\"+node['parent'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1\t2,3|0|BLACK|source\r\n",
      "reporter:counter:example-info,black_count,1\r\n",
      "2\tnull|1|GRAY|1\r\n",
      "3\tnull|1|GRAY|1\r\n",
      "2\t1,3,4,5|Integer.MAX_VALUE|WHITE|null\r\n",
      "3\t1,4,2|Integer.MAX_VALUE|WHITE|null\r\n",
      "4\t2,3|Integer.MAX_VALUE|WHITE|null\r\n",
      "5\t2|Integer.MAX_VALUE|WHITE|null\r\n"
     ]
    }
   ],
   "source": [
    "!cat data2/testbfs.txt | python mapper.py "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Deleted /outputs\r\n"
     ]
    }
   ],
   "source": [
    "!hdfs dfs -rm -R /outputs\n",
    "!hadoop jar /opt/hadoop-3.2.1/share/hadoop/tools/lib/hadoop-streaming-3.2.1.jar \\\n",
    "    -D mapreduce.job.reduces=5 \\\n",
    "    -file $PWD/mapper.py\\\n",
    "    -file $PWD/reducer.py\\\n",
    "    -mapper mapper.py \\\n",
    "    -reducer reducer.py \\\n",
    "    -input /inputs/data2/int3.txt \\\n",
    "    -output /outputs/out > /dev/null 2> err.txt \\\n",
    "    -partitioner org.apache.hadoop.mapred.lib.KeyFieldBasedPartitioner"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/dev/null\r\n"
     ]
    }
   ],
   "source": [
    "!cat err.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 6 items\r\n",
      "-rw-r--r--   3 root supergroup          0 2020-10-23 00:20 /outputs/out/_SUCCESS\r\n",
      "-rw-r--r--   3 root supergroup         20 2020-10-23 00:19 /outputs/out/part-00000\r\n",
      "-rw-r--r--   3 root supergroup         18 2020-10-23 00:20 /outputs/out/part-00001\r\n",
      "-rw-r--r--   3 root supergroup         16 2020-10-23 00:20 /outputs/out/part-00002\r\n",
      "-rw-r--r--   3 root supergroup         14 2020-10-23 00:20 /outputs/out/part-00003\r\n",
      "-rw-r--r--   3 root supergroup         21 2020-10-23 00:20 /outputs/out/part-00004\r\n"
     ]
    }
   ],
   "source": [
    "!hdfs dfs -ls /outputs/out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2020-10-22 23:46:11,770 INFO sasl.SaslDataTransferClient: SASL encryption trust check: localHostTrusted = false, remoteHostTrusted = false\n",
      "2\t1,3,4,5|1|BLACK|1\n",
      "2020-10-22 23:46:16,014 INFO sasl.SaslDataTransferClient: SASL encryption trust check: localHostTrusted = false, remoteHostTrusted = false\n",
      "3\t1,4,2|1|BLACK|1\n",
      "2020-10-22 23:46:20,498 INFO sasl.SaslDataTransferClient: SASL encryption trust check: localHostTrusted = false, remoteHostTrusted = false\n",
      "4\t2,3|2|BLACK|2\n",
      "2020-10-22 23:46:24,194 INFO sasl.SaslDataTransferClient: SASL encryption trust check: localHostTrusted = false, remoteHostTrusted = false\n",
      "5\t2|2|BLACK|2\n",
      "2020-10-22 23:46:28,783 INFO sasl.SaslDataTransferClient: SASL encryption trust check: localHostTrusted = false, remoteHostTrusted = false\n",
      "1\t2,3|0|BLACK|source\n"
     ]
    }
   ],
   "source": [
    "!hdfs dfs -cat /outputs/result2/part-00000\n",
    "!hdfs dfs -cat /outputs/result2/part-00001\n",
    "!hdfs dfs -cat /outputs/result2/part-00002\n",
    "!hdfs dfs -cat /outputs/result2/part-00003\n",
    "!hdfs dfs -cat /outputs/result2/part-00004"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2020-10-22 23:53:26,836 INFO sasl.SaslDataTransferClient: SASL encryption trust check: localHostTrusted = false, remoteHostTrusted = false\n",
      "2\t1,3,4,5|1|BLACK|1\n",
      "3\t1,4,2|1|BLACK|1\n",
      "4\t2,3|2|BLACK|2\n",
      "5\t2|2|BLACK|2\n",
      "1\t2,3|0|BLACK|source\n"
     ]
    }
   ],
   "source": [
    "!hdfs dfs -cat /outputs/result2/part-00000  /outputs/result2/part-00001 /outputs/result2/part-00002 /outputs/result2/part-00003 /outputs/result2/part-00004  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Final Output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Writing script.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile script.py\n",
    "#!/opt/bitnami/python/bin/python\n",
    "# -*-coding:utf-8 -*\n",
    "import os\n",
    "\n",
    "black_count=0\n",
    "i=1\n",
    "o=2\n",
    "while(black_count<5):\n",
    "    print(\"black_count=\"+str(black_count))\n",
    "    print(\"interation=\"+str(i))\n",
    "#     black_count=5#black_count+1\n",
    "    cmd1 = 'hadoop jar /opt/hadoop-3.2.1/share/hadoop/tools/lib/hadoop-streaming-3.2.1.jar \\\n",
    "    -D mapreduce.job.reduces=5 \\\n",
    "    -file $PWD/mapper.py\\\n",
    "    -file $PWD/reducer.py\\\n",
    "    -mapper mapper.py \\\n",
    "    -reducer reducer.py \\\n",
    "    -input /h{} \\\n",
    "    -output /h{} > /dev/null 2> err.txt \\\n",
    "    -partitioner org.apache.hadoop.mapred.lib.KeyFieldBasedPartitioner'.format(str(i),str(o))   \n",
    "    os.system(cmd1)\n",
    "    word = \"black_count\"\n",
    "    with open(\"err.txt\") as search:\n",
    "        for line in search:\n",
    "            line = line.rstrip()  # remove '\\n' at end of line\n",
    "            if word in line:\n",
    "                black_count = int(line[-1])\n",
    "    i = i + 1\n",
    "    o = o + 1\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "black_count=0\n",
      "interation=1\n",
      "black_count=1\n",
      "interation=2\n",
      "black_count=3\n",
      "interation=3\n"
     ]
    }
   ],
   "source": [
    "!python script.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING: Use of this script to execute dfs is deprecated.\n",
      "WARNING: Attempting to execute replacement \"hdfs dfs\" instead.\n",
      "\n",
      "Found 6 items\n",
      "-rw-r--r--   3 root supergroup          0 2020-10-25 05:24 /h4/_SUCCESS\n",
      "-rw-r--r--   3 root supergroup         20 2020-10-25 05:23 /h4/part-00000\n",
      "-rw-r--r--   3 root supergroup         18 2020-10-25 05:24 /h4/part-00001\n",
      "-rw-r--r--   3 root supergroup         16 2020-10-25 05:24 /h4/part-00002\n",
      "-rw-r--r--   3 root supergroup         14 2020-10-25 05:24 /h4/part-00003\n",
      "-rw-r--r--   3 root supergroup         21 2020-10-25 05:24 /h4/part-00004\n"
     ]
    }
   ],
   "source": [
    "!hadoop dfs -ls /h4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING: Use of this script to execute dfs is deprecated.\n",
      "WARNING: Attempting to execute replacement \"hdfs dfs\" instead.\n",
      "\n",
      "2020-10-25 05:32:44,710 INFO sasl.SaslDataTransferClient: SASL encryption trust check: localHostTrusted = false, remoteHostTrusted = false\n",
      "2\t1,3,4,5|1|BLACK|1\n",
      "WARNING: Use of this script to execute dfs is deprecated.\n",
      "WARNING: Attempting to execute replacement \"hdfs dfs\" instead.\n",
      "\n",
      "2020-10-25 05:32:48,666 INFO sasl.SaslDataTransferClient: SASL encryption trust check: localHostTrusted = false, remoteHostTrusted = false\n",
      "3\t1,4,2|1|BLACK|1\n",
      "WARNING: Use of this script to execute dfs is deprecated.\n",
      "WARNING: Attempting to execute replacement \"hdfs dfs\" instead.\n",
      "\n",
      "2020-10-25 05:32:52,419 INFO sasl.SaslDataTransferClient: SASL encryption trust check: localHostTrusted = false, remoteHostTrusted = false\n",
      "4\t2,3|2|BLACK|2\n",
      "WARNING: Use of this script to execute dfs is deprecated.\n",
      "WARNING: Attempting to execute replacement \"hdfs dfs\" instead.\n",
      "\n",
      "2020-10-25 05:32:58,107 INFO sasl.SaslDataTransferClient: SASL encryption trust check: localHostTrusted = false, remoteHostTrusted = false\n",
      "5\t2|2|BLACK|2\n",
      "WARNING: Use of this script to execute dfs is deprecated.\n",
      "WARNING: Attempting to execute replacement \"hdfs dfs\" instead.\n",
      "\n",
      "2020-10-25 05:33:01,977 INFO sasl.SaslDataTransferClient: SASL encryption trust check: localHostTrusted = false, remoteHostTrusted = false\n",
      "1\t2,3|0|BLACK|source\n"
     ]
    }
   ],
   "source": [
    "!hadoop dfs -cat /h4/part-00000\n",
    "!hadoop dfs -cat /h4/part-00001\n",
    "!hadoop dfs -cat /h4/part-00002\n",
    "!hadoop dfs -cat /h4/part-00003\n",
    "!hadoop dfs -cat /h4/part-00004"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
