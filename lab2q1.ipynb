{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0,0,893480\r\n",
      "0,1,602014\r\n",
      "0,2,161301\r\n",
      "0,3,310639\r\n",
      "0,4,758183\r\n",
      "cat: write error: Broken pipe\r\n"
     ]
    }
   ],
   "source": [
    "!cat data1/matrix.txt | head -n 5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Deleted /outputs\n",
      "Deleted /inputs\n"
     ]
    }
   ],
   "source": [
    "!hdfs dfs -rm -R /outputs\n",
    "!hdfs dfs -rm -R /inputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2020-10-21 21:35:13,787 INFO sasl.SaslDataTransferClient: SASL encryption trust check: localHostTrusted = false, remoteHostTrusted = false\n",
      "2020-10-21 21:35:19,143 INFO sasl.SaslDataTransferClient: SASL encryption trust check: localHostTrusted = false, remoteHostTrusted = false\n"
     ]
    }
   ],
   "source": [
    "!hadoop fs -mkdir -p /outputs\n",
    "!hadoop fs -mkdir -p /inputs\n",
    "!hadoop fs -put data1 /inputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING: Use of this script to execute dfs is deprecated.\n",
      "WARNING: Attempting to execute replacement \"hdfs dfs\" instead.\n",
      "\n",
      "Found 2 items\n",
      "-rw-r--r--   3 root supergroup   46785000 2020-10-21 21:35 /inputs/data1/matrix.txt\n",
      "-rw-r--r--   3 root supergroup         85 2020-10-21 21:35 /inputs/data1/smatrix.txt\n"
     ]
    }
   ],
   "source": [
    "!hadoop dfs -ls /inputs/data1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting mapper.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile mapper.py\n",
    "#!/opt/bitnami/python/bin/python\n",
    "# -*-coding:utf-8 -*\n",
    "import sys\n",
    "import string\n",
    "\n",
    "for line in sys.stdin: # reads from stdin\n",
    "    line = line.strip()\n",
    "    row, col, val = line.split(\",\")\n",
    "    print(row+\"\\t\"+col+\",\"+val)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting reducer.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile reducer.py\n",
    "#!/opt/bitnami/python/bin/python\n",
    "# -*-coding:utf-8 -*\n",
    "\n",
    "import sys\n",
    "import random\n",
    "\n",
    "for line in sys.stdin:\n",
    "    line = line.strip()\n",
    "    row, colval = line.split()\n",
    "    col, val = colval.split(\",\")\n",
    "    print(col+\" \"+row+\" \"+val)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 0 1\r\n",
      "1 0 2\r\n",
      "2 0 3\r\n",
      "3 0 4\r\n",
      "0 1 5\r\n",
      "1 1 6\r\n",
      "2 1 7\r\n",
      "3 2 8\r\n",
      "0 2 9\r\n",
      "1 2 10\r\n",
      "2 2 11\r\n",
      "3 2 12\r\n"
     ]
    }
   ],
   "source": [
    "!cat data1/smatrix.txt | python mapper.py | python reducer.py"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Small Matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2020-10-21 21:36:25,072 WARN streaming.StreamJob: -file option is deprecated, please use generic option -files instead.\n",
      "packageJobJar: [/training/mapper.py, /training/reducer.py, /tmp/hadoop-unjar2874859834494374989/] [] /tmp/streamjob7512282812789142905.jar tmpDir=null\n",
      "2020-10-21 21:36:28,805 INFO client.RMProxy: Connecting to ResourceManager at resourcemanager/172.18.0.4:8032\n",
      "2020-10-21 21:36:30,118 INFO client.AHSProxy: Connecting to Application History server at historyserver/172.18.0.5:10200\n",
      "2020-10-21 21:36:30,410 INFO client.RMProxy: Connecting to ResourceManager at resourcemanager/172.18.0.4:8032\n",
      "2020-10-21 21:36:30,410 INFO client.AHSProxy: Connecting to Application History server at historyserver/172.18.0.5:10200\n",
      "2020-10-21 21:36:31,880 INFO mapreduce.JobResourceUploader: Disabling Erasure Coding for path: /tmp/hadoop-yarn/staging/root/.staging/job_1603314927371_0001\n",
      "2020-10-21 21:36:33,091 INFO sasl.SaslDataTransferClient: SASL encryption trust check: localHostTrusted = false, remoteHostTrusted = false\n",
      "2020-10-21 21:36:34,057 INFO sasl.SaslDataTransferClient: SASL encryption trust check: localHostTrusted = false, remoteHostTrusted = false\n",
      "2020-10-21 21:36:34,891 INFO sasl.SaslDataTransferClient: SASL encryption trust check: localHostTrusted = false, remoteHostTrusted = false\n",
      "2020-10-21 21:36:35,520 INFO mapred.FileInputFormat: Total input files to process : 1\n",
      "2020-10-21 21:36:36,147 INFO sasl.SaslDataTransferClient: SASL encryption trust check: localHostTrusted = false, remoteHostTrusted = false\n",
      "2020-10-21 21:36:37,392 INFO sasl.SaslDataTransferClient: SASL encryption trust check: localHostTrusted = false, remoteHostTrusted = false\n",
      "2020-10-21 21:36:37,553 INFO mapreduce.JobSubmitter: number of splits:2\n",
      "2020-10-21 21:36:40,233 INFO sasl.SaslDataTransferClient: SASL encryption trust check: localHostTrusted = false, remoteHostTrusted = false\n",
      "2020-10-21 21:36:40,812 INFO mapreduce.JobSubmitter: Submitting tokens for job: job_1603314927371_0001\n",
      "2020-10-21 21:36:40,812 INFO mapreduce.JobSubmitter: Executing with tokens: []\n",
      "2020-10-21 21:36:42,186 INFO conf.Configuration: resource-types.xml not found\n",
      "2020-10-21 21:36:42,186 INFO resource.ResourceUtils: Unable to find 'resource-types.xml'.\n",
      "2020-10-21 21:36:51,646 INFO impl.YarnClientImpl: Submitted application application_1603314927371_0001\n",
      "2020-10-21 21:36:51,943 INFO mapreduce.Job: The url to track the job: http://resourcemanager:8088/proxy/application_1603314927371_0001/\n",
      "2020-10-21 21:36:51,961 INFO mapreduce.Job: Running job: job_1603314927371_0001\n",
      "2020-10-21 21:38:59,164 INFO mapreduce.Job: Job job_1603314927371_0001 running in uber mode : false\n",
      "2020-10-21 21:38:59,319 INFO mapreduce.Job:  map 0% reduce 0%\n",
      "2020-10-21 21:40:36,543 INFO mapreduce.Job:  map 100% reduce 0%\n",
      "2020-10-21 21:41:30,993 INFO mapreduce.Job:  map 100% reduce 33%\n",
      "2020-10-21 21:41:40,067 INFO mapreduce.Job:  map 100% reduce 67%\n",
      "2020-10-21 21:41:47,112 INFO mapreduce.Job:  map 100% reduce 100%\n",
      "2020-10-21 21:41:56,194 INFO mapreduce.Job: Job job_1603314927371_0001 completed successfully\n",
      "2020-10-21 21:41:56,597 INFO mapreduce.Job: Counters: 55\n",
      "\tFile System Counters\n",
      "\t\tFILE: Number of bytes read=141\n",
      "\t\tFILE: Number of bytes written=1165306\n",
      "\t\tFILE: Number of read operations=0\n",
      "\t\tFILE: Number of large read operations=0\n",
      "\t\tFILE: Number of write operations=0\n",
      "\t\tHDFS: Number of bytes read=322\n",
      "\t\tHDFS: Number of bytes written=87\n",
      "\t\tHDFS: Number of read operations=21\n",
      "\t\tHDFS: Number of large read operations=0\n",
      "\t\tHDFS: Number of write operations=6\n",
      "\t\tHDFS: Number of bytes read erasure-coded=0\n",
      "\tJob Counters \n",
      "\t\tKilled reduce tasks=1\n",
      "\t\tLaunched map tasks=2\n",
      "\t\tLaunched reduce tasks=3\n",
      "\t\tRack-local map tasks=2\n",
      "\t\tTotal time spent by all maps in occupied slots (ms)=626984\n",
      "\t\tTotal time spent by all reduces in occupied slots (ms)=400776\n",
      "\t\tTotal time spent by all map tasks (ms)=156746\n",
      "\t\tTotal time spent by all reduce tasks (ms)=50097\n",
      "\t\tTotal vcore-milliseconds taken by all map tasks=156746\n",
      "\t\tTotal vcore-milliseconds taken by all reduce tasks=50097\n",
      "\t\tTotal megabyte-milliseconds taken by all map tasks=642031616\n",
      "\t\tTotal megabyte-milliseconds taken by all reduce tasks=410394624\n",
      "\tMap-Reduce Framework\n",
      "\t\tMap input records=12\n",
      "\t\tMap output records=12\n",
      "\t\tMap output bytes=75\n",
      "\t\tMap output materialized bytes=159\n",
      "\t\tInput split bytes=194\n",
      "\t\tCombine input records=0\n",
      "\t\tCombine output records=0\n",
      "\t\tReduce input groups=3\n",
      "\t\tReduce shuffle bytes=159\n",
      "\t\tReduce input records=12\n",
      "\t\tReduce output records=12\n",
      "\t\tSpilled Records=24\n",
      "\t\tShuffled Maps =6\n",
      "\t\tFailed Shuffles=0\n",
      "\t\tMerged Map outputs=6\n",
      "\t\tGC time elapsed (ms)=1268\n",
      "\t\tCPU time spent (ms)=6310\n",
      "\t\tPhysical memory (bytes) snapshot=864694272\n",
      "\t\tVirtual memory (bytes) snapshot=35185496064\n",
      "\t\tTotal committed heap usage (bytes)=585629696\n",
      "\t\tPeak Map Physical memory (bytes)=394809344\n",
      "\t\tPeak Map Virtual memory (bytes)=10047561728\n",
      "\t\tPeak Reduce Physical memory (bytes)=172146688\n",
      "\t\tPeak Reduce Virtual memory (bytes)=8377118720\n",
      "\tShuffle Errors\n",
      "\t\tBAD_ID=0\n",
      "\t\tCONNECTION=0\n",
      "\t\tIO_ERROR=0\n",
      "\t\tWRONG_LENGTH=0\n",
      "\t\tWRONG_MAP=0\n",
      "\t\tWRONG_REDUCE=0\n",
      "\tFile Input Format Counters \n",
      "\t\tBytes Read=128\n",
      "\tFile Output Format Counters \n",
      "\t\tBytes Written=87\n",
      "2020-10-21 21:41:56,597 INFO streaming.StreamJob: Output directory: /outputs/result1\n"
     ]
    }
   ],
   "source": [
    "!hadoop jar /opt/hadoop-3.2.1/share/hadoop/tools/lib/hadoop-streaming-3.2.1.jar \\\n",
    "    -D mapreduce.job.reduces=3 \\\n",
    "    -file $PWD/mapper.py\\\n",
    "    -file $PWD/reducer.py\\\n",
    "    -mapper mapper.py \\\n",
    "    -reducer reducer.py \\\n",
    "    -input /inputs/data1/smatrix.txt \\\n",
    "    -output /outputs/result1 \\\n",
    "    -partitioner org.apache.hadoop.mapred.lib.KeyFieldBasedPartitioner"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Deleted /outputs/result1\r\n"
     ]
    }
   ],
   "source": [
    "!hdfs dfs -rm -R /outputs/result1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 4 items\r\n",
      "-rw-r--r--   3 root supergroup          0 2020-10-21 21:41 /outputs/result1/_SUCCESS\r\n",
      "-rw-r--r--   3 root supergroup         28 2020-10-21 21:41 /outputs/result1/part-00000\r\n",
      "-rw-r--r--   3 root supergroup         21 2020-10-21 21:41 /outputs/result1/part-00001\r\n",
      "-rw-r--r--   3 root supergroup         38 2020-10-21 21:41 /outputs/result1/part-00002\r\n"
     ]
    }
   ],
   "source": [
    "!hdfs dfs -ls /outputs/result1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2020-10-21 21:50:48,093 INFO sasl.SaslDataTransferClient: SASL encryption trust check: localHostTrusted = false, remoteHostTrusted = false\n",
      "3 0 4\t\n",
      "2 0 3\t\n",
      "1 0 2\t\n",
      "0 0 1\t\n"
     ]
    }
   ],
   "source": [
    "!hdfs dfs -cat /outputs/result1/part-00000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2020-10-21 21:50:53,926 INFO sasl.SaslDataTransferClient: SASL encryption trust check: localHostTrusted = false, remoteHostTrusted = false\n",
      "2 1 7\t\n",
      "1 1 6\t\n",
      "0 1 5\t\n"
     ]
    }
   ],
   "source": [
    "!hdfs dfs -cat /outputs/result1/part-00001"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2020-10-21 21:51:03,484 INFO sasl.SaslDataTransferClient: SASL encryption trust check: localHostTrusted = false, remoteHostTrusted = false\n",
      "3 2 12\t\n",
      "2 2 11\t\n",
      "1 2 10\t\n",
      "0 2 9\t\n",
      "3 2 8\t\n"
     ]
    }
   ],
   "source": [
    "!hdfs dfs -cat /outputs/result1/part-00002"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cat: `/outputs/result1/part-00003': No such file or directory\r\n"
     ]
    }
   ],
   "source": [
    "!hdfs dfs -cat /outputs/result1/part-00003"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Big Matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Deleted /outputs\n",
      "2020-10-21 21:51:34,930 WARN streaming.StreamJob: -file option is deprecated, please use generic option -files instead.\n",
      "packageJobJar: [/training/mapper.py, /training/reducer.py, /tmp/hadoop-unjar1284866028686982399/] [] /tmp/streamjob9049864868948939990.jar tmpDir=null\n",
      "2020-10-21 21:51:38,473 INFO client.RMProxy: Connecting to ResourceManager at resourcemanager/172.18.0.4:8032\n",
      "2020-10-21 21:51:44,473 INFO client.AHSProxy: Connecting to Application History server at historyserver/172.18.0.5:10200\n",
      "2020-10-21 21:51:44,577 INFO client.RMProxy: Connecting to ResourceManager at resourcemanager/172.18.0.4:8032\n",
      "2020-10-21 21:51:44,577 INFO client.AHSProxy: Connecting to Application History server at historyserver/172.18.0.5:10200\n",
      "2020-10-21 21:51:45,897 INFO mapreduce.JobResourceUploader: Disabling Erasure Coding for path: /tmp/hadoop-yarn/staging/root/.staging/job_1603314927371_0002\n",
      "2020-10-21 21:51:46,881 INFO sasl.SaslDataTransferClient: SASL encryption trust check: localHostTrusted = false, remoteHostTrusted = false\n",
      "2020-10-21 21:51:47,524 INFO sasl.SaslDataTransferClient: SASL encryption trust check: localHostTrusted = false, remoteHostTrusted = false\n",
      "2020-10-21 21:51:47,913 INFO sasl.SaslDataTransferClient: SASL encryption trust check: localHostTrusted = false, remoteHostTrusted = false\n",
      "2020-10-21 21:51:48,294 INFO mapred.FileInputFormat: Total input files to process : 1\n",
      "2020-10-21 21:51:49,525 INFO sasl.SaslDataTransferClient: SASL encryption trust check: localHostTrusted = false, remoteHostTrusted = false\n",
      "2020-10-21 21:51:49,969 INFO sasl.SaslDataTransferClient: SASL encryption trust check: localHostTrusted = false, remoteHostTrusted = false\n",
      "2020-10-21 21:51:50,080 INFO mapreduce.JobSubmitter: number of splits:2\n",
      "2020-10-21 21:51:50,614 INFO sasl.SaslDataTransferClient: SASL encryption trust check: localHostTrusted = false, remoteHostTrusted = false\n",
      "2020-10-21 21:51:51,191 INFO mapreduce.JobSubmitter: Submitting tokens for job: job_1603314927371_0002\n",
      "2020-10-21 21:51:51,191 INFO mapreduce.JobSubmitter: Executing with tokens: []\n",
      "2020-10-21 21:51:52,838 INFO conf.Configuration: resource-types.xml not found\n",
      "2020-10-21 21:51:52,840 INFO resource.ResourceUtils: Unable to find 'resource-types.xml'.\n",
      "2020-10-21 21:51:55,062 INFO impl.YarnClientImpl: Submitted application application_1603314927371_0002\n",
      "2020-10-21 21:51:55,208 INFO mapreduce.Job: The url to track the job: http://resourcemanager:8088/proxy/application_1603314927371_0002/\n",
      "2020-10-21 21:51:55,211 INFO mapreduce.Job: Running job: job_1603314927371_0002\n",
      "2020-10-21 21:52:17,747 INFO mapreduce.Job: Job job_1603314927371_0002 running in uber mode : false\n",
      "2020-10-21 21:52:17,798 INFO mapreduce.Job:  map 0% reduce 0%\n",
      "2020-10-21 21:53:38,859 INFO mapreduce.Job:  map 15% reduce 0%\n",
      "2020-10-21 21:53:44,957 INFO mapreduce.Job:  map 25% reduce 0%\n",
      "2020-10-21 21:53:51,037 INFO mapreduce.Job:  map 39% reduce 0%\n",
      "2020-10-21 21:53:57,543 INFO mapreduce.Job:  map 52% reduce 0%\n",
      "2020-10-21 21:54:03,220 INFO mapreduce.Job:  map 66% reduce 0%\n",
      "2020-10-21 21:54:09,594 INFO mapreduce.Job:  map 67% reduce 0%\n",
      "2020-10-21 21:54:27,138 INFO mapreduce.Job:  map 100% reduce 0%\n",
      "2020-10-21 21:56:34,385 INFO mapreduce.Job:  map 100% reduce 10%\n",
      "2020-10-21 21:56:45,516 INFO mapreduce.Job:  map 100% reduce 20%\n",
      "2020-10-21 21:56:59,641 INFO mapreduce.Job:  map 100% reduce 30%\n",
      "2020-10-21 21:57:10,745 INFO mapreduce.Job:  map 100% reduce 40%\n",
      "2020-10-21 21:57:20,835 INFO mapreduce.Job:  map 100% reduce 50%\n",
      "2020-10-21 21:57:32,921 INFO mapreduce.Job:  map 100% reduce 60%\n",
      "2020-10-21 21:57:46,997 INFO mapreduce.Job:  map 100% reduce 70%\n",
      "2020-10-21 21:57:58,342 INFO mapreduce.Job:  map 100% reduce 80%\n",
      "2020-10-21 21:58:09,394 INFO mapreduce.Job:  map 100% reduce 90%\n",
      "2020-10-21 21:58:26,628 INFO mapreduce.Job:  map 100% reduce 100%\n",
      "2020-10-21 21:59:21,993 INFO mapreduce.Job: Job job_1603314927371_0002 completed successfully\n",
      "2020-10-21 21:59:36,500 INFO mapreduce.Job: Counters: 55\n",
      "\tFile System Counters\n",
      "\t\tFILE: Number of bytes read=18257508\n",
      "\t\tFILE: Number of bytes written=39314535\n",
      "\t\tFILE: Number of read operations=0\n",
      "\t\tFILE: Number of large read operations=0\n",
      "\t\tFILE: Number of write operations=0\n",
      "\t\tHDFS: Number of bytes read=46789044\n",
      "\t\tHDFS: Number of bytes written=49785000\n",
      "\t\tHDFS: Number of read operations=56\n",
      "\t\tHDFS: Number of large read operations=0\n",
      "\t\tHDFS: Number of write operations=20\n",
      "\t\tHDFS: Number of bytes read erasure-coded=0\n",
      "\tJob Counters \n",
      "\t\tKilled reduce tasks=1\n",
      "\t\tLaunched map tasks=2\n",
      "\t\tLaunched reduce tasks=10\n",
      "\t\tRack-local map tasks=2\n",
      "\t\tTotal time spent by all maps in occupied slots (ms)=995496\n",
      "\t\tTotal time spent by all reduces in occupied slots (ms)=1450208\n",
      "\t\tTotal time spent by all map tasks (ms)=248874\n",
      "\t\tTotal time spent by all reduce tasks (ms)=181276\n",
      "\t\tTotal vcore-milliseconds taken by all map tasks=248874\n",
      "\t\tTotal vcore-milliseconds taken by all reduce tasks=181276\n",
      "\t\tTotal megabyte-milliseconds taken by all map tasks=1019387904\n",
      "\t\tTotal megabyte-milliseconds taken by all reduce tasks=1485012992\n",
      "\tMap-Reduce Framework\n",
      "\t\tMap input records=3000000\n",
      "\t\tMap output records=3000000\n",
      "\t\tMap output bytes=46785000\n",
      "\t\tMap output materialized bytes=18260965\n",
      "\t\tInput split bytes=192\n",
      "\t\tCombine input records=0\n",
      "\t\tCombine output records=0\n",
      "\t\tReduce input groups=500\n",
      "\t\tReduce shuffle bytes=18260965\n",
      "\t\tReduce input records=3000000\n",
      "\t\tReduce output records=3000000\n",
      "\t\tSpilled Records=6000000\n",
      "\t\tShuffled Maps =20\n",
      "\t\tFailed Shuffles=0\n",
      "\t\tMerged Map outputs=20\n",
      "\t\tGC time elapsed (ms)=5459\n",
      "\t\tCPU time spent (ms)=79620\n",
      "\t\tPhysical memory (bytes) snapshot=2496221184\n",
      "\t\tVirtual memory (bytes) snapshot=93813448704\n",
      "\t\tTotal committed heap usage (bytes)=1664090112\n",
      "\t\tPeak Map Physical memory (bytes)=255205376\n",
      "\t\tPeak Map Virtual memory (bytes)=5066039296\n",
      "\t\tPeak Reduce Physical memory (bytes)=222576640\n",
      "\t\tPeak Reduce Virtual memory (bytes)=8381423616\n",
      "\tShuffle Errors\n",
      "\t\tBAD_ID=0\n",
      "\t\tCONNECTION=0\n",
      "\t\tIO_ERROR=0\n",
      "\t\tWRONG_LENGTH=0\n",
      "\t\tWRONG_MAP=0\n",
      "\t\tWRONG_REDUCE=0\n",
      "\tFile Input Format Counters \n",
      "\t\tBytes Read=46788852\n",
      "\tFile Output Format Counters \n",
      "\t\tBytes Written=49785000\n",
      "2020-10-21 21:59:36,538 INFO streaming.StreamJob: Output directory: /outputs/result1\n"
     ]
    }
   ],
   "source": [
    "!hdfs dfs -rm -R /outputs\n",
    "!hadoop jar /opt/hadoop-3.2.1/share/hadoop/tools/lib/hadoop-streaming-3.2.1.jar \\\n",
    "    -D mapreduce.job.reduces=10 \\\n",
    "    -file $PWD/mapper.py\\\n",
    "    -file $PWD/reducer.py\\\n",
    "    -mapper mapper.py \\\n",
    "    -reducer reducer.py \\\n",
    "    -input /inputs/data1/matrix.txt \\\n",
    "    -output /outputs/result1 \\\n",
    "    -partitioner org.apache.hadoop.mapred.lib.KeyFieldBasedPartitioner"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 11 items\r\n",
      "-rw-r--r--   3 root supergroup          0 2020-10-21 21:58 /outputs/result1/_SUCCESS\r\n",
      "-rw-r--r--   3 root supergroup    4978500 2020-10-21 21:56 /outputs/result1/part-00000\r\n",
      "-rw-r--r--   3 root supergroup    4978500 2020-10-21 21:56 /outputs/result1/part-00001\r\n",
      "-rw-r--r--   3 root supergroup    4978500 2020-10-21 21:56 /outputs/result1/part-00002\r\n",
      "-rw-r--r--   3 root supergroup    4978500 2020-10-21 21:57 /outputs/result1/part-00003\r\n",
      "-rw-r--r--   3 root supergroup    4978500 2020-10-21 21:57 /outputs/result1/part-00004\r\n",
      "-rw-r--r--   3 root supergroup    4978500 2020-10-21 21:57 /outputs/result1/part-00005\r\n",
      "-rw-r--r--   3 root supergroup    4978500 2020-10-21 21:57 /outputs/result1/part-00006\r\n",
      "-rw-r--r--   3 root supergroup    4978500 2020-10-21 21:57 /outputs/result1/part-00007\r\n",
      "-rw-r--r--   3 root supergroup    4978500 2020-10-21 21:58 /outputs/result1/part-00008\r\n",
      "-rw-r--r--   3 root supergroup    4978500 2020-10-21 21:58 /outputs/result1/part-00009\r\n"
     ]
    }
   ],
   "source": [
    "!hdfs dfs -ls /outputs/result1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2020-10-21 22:00:45,228 INFO sasl.SaslDataTransferClient: SASL encryption trust check: localHostTrusted = false, remoteHostTrusted = false\n",
      "2902 106 173883\t\n",
      "5999 106 138702\t\n",
      "5998 106 843380\t\n",
      "5997 106 151487\t\n",
      "5996 106 328035\t\n",
      "5995 106 456071\t\n",
      "5994 106 305879\t\n",
      "5993 106 744884\t\n",
      "5992 106 479108\t\n",
      "5991 106 156839\t\n",
      "cat: Unable to write to output stream.\n"
     ]
    }
   ],
   "source": [
    "!hdfs dfs -cat /outputs/result1/part-00001 | head"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2020-10-21 22:00:52,726 INFO sasl.SaslDataTransferClient: SASL encryption trust check: localHostTrusted = false, remoteHostTrusted = false\n",
      "2912 97 654964\t\n",
      "2911 97 145313\t\n",
      "2910 97 199041\t\n",
      "2909 97 710664\t\n",
      "2908 97 370762\t\n",
      "2907 97 686962\t\n",
      "2906 97 606642\t\n",
      "2905 97 215659\t\n",
      "2904 97 376380\t\n",
      "2903 97 760455\t\n"
     ]
    }
   ],
   "source": [
    "!hdfs dfs -cat /outputs/result1/part-00002 | tail"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
